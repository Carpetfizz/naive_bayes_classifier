{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "This notebook is a companion to [Note 21, Inference](http://www.eecs70.org/static/notes/n21.pdf) of the [CS70](http://www.eecs70.org) lecture notes. Much of this notebook is a paraphrase of the material presented in the original note, along with my own annotations and live code examples.\n",
    "\n",
    "Trying to classify digits by trying to teach a computer specific rules about what a $1$ or a $2$ looks like is difficult and does not scale well. Try writing a $5$ multiple times on a piece of paper, do they all appear exactly the same? It is difficult to classify your own handwriting, let alone classify the handwriting of some arbitrary person. Instead we will use probability theory, specifically Bayes Rule, to build a simple classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "$\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{arg}\\,\\operatorname{max}}\\;}$\n",
    "For each digit $j \\in \\{0,1,...9\\}$, we define the probability distribution $P_j(x)$. I will clarify this notation soon. If we treat image classification as a balls and bins problems, it's clear that there are $10$ bins, each labeled some $j$. The balls in each bin are the images of the digit corresponding to that particular bin.\n",
    "\n",
    " ![balls_bins](images/balls_bins.png)\n",
    " \n",
    "Each of these bins is labeled \"prior $\\pi_j$\" because the distribution $P(y=j) = \\pi_j,\\,j \\in \\{0,1,...9\\}$ is known as a the \"prior distribution\". It represents our current understanding of the scenario. It may not be clear where this distribution comes from, we will address that soon. It may be clear now, that classification is the same as a balls and bins problem where we are given a ball and we need to guess which bin it came from.\n",
    "\n",
    "## Classification as inference\n",
    " \n",
    "We can rewrite $P_j(x) = P(x\\mid y=j)$ which translates to \"the probability of generating image $x$ given that bin label $y$ is $j$\". In other words it's the probability that a machine will generate image $x$ of digit $j$. It is not clear where $P_j(x)$ comes from either, but we will address that soon. However, we do have all the necessary components to answer the question, \"Given image $x$, what's the probability that bin label $y$ is $j$?\". Convince yourself that answering this question answers the digit classification problem.\n",
    "\n",
    "\\begin{align*}\n",
    "    P(y=j \\mid x) = \\frac{P(y=j, x)}{P(x)} = \\frac{P(x \\mid y=j) \\cdot P(y=j)}{P(x)} = \\frac{\\pi_jP_j(x)}{P(x)} = \\frac{\\pi_jP_j(x)}{\\sum_{i=0}^9 P(x, y=i)} = \\frac{\\pi_jP_j(x)}{\\sum_{i=0}^9 P(x|y=i)P(y=i)} = \\frac{\\pi_jP_j(x)}{\\sum_{i=0}^9 P_i(x)\\pi_i}\n",
    "\\end{align*}\n",
    "\n",
    "The third inequality uses Bayes Rule and the fifth inequality uses the Law of Total Probability.\n",
    "\n",
    "Now, all we have to do is find the digit $j$ that maximizes the probability $P(y=j \\mid x)$. Note that $P(y=j \\mid x)$ is the *posterior* distribution. It represents the knowledge we have after using the Bayes Rule with the *prior* distribution $P(y=j)$. Let $h(x)$ be the digit we predict given image $x$.\n",
    "\n",
    "\\begin{align*}\n",
    "h(x) &= \\arg\\max_{j} \\pi_jP_j(x) \\\\\n",
    "     &= \\arg\\max_{j} \\frac{\\pi_jP_j(x)}{\\sum_{i=0}^9 P_i(x)\\pi_i} \\\\\n",
    "     &= \\arg\\max_{j} \\pi_jP_j(x)\n",
    "\\end{align*}\n",
    "\n",
    "This syntax means \"$h(x)$ is the argument $j$ which maximizes $\\pi_jP_j(x)$\". Note that the denominator for $P(y=j \\mid x)$ does not depend on $j$ so we don't need to consider it when maximizing $j$. This is called the *maximum a posteriori* (MAP) estimator.\n",
    "\n",
    "We now have formal mathematical model for inferring a digit $h(x)$ given an image $x$. Now, all we have to do is find a way to compute $\\pi_j$ (prior distribution) and $P_j(x)$ (conditional distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating the Prior Distribution\n",
    "\n",
    "Given a data set of $n$ $(x,y)$ pairs where $x$ is an image and $y$ is the corresponding label (digit), we can \"train\" the model to provide accurate probability estimations of each digit, given an image. Training is essentially estimating $P(y=j) = \\pi_j$, the prior distribution, and estimating $P(x \\mid y=j) = P_j(x)$, the conditional distribution.\n",
    "\n",
    "Let $n$ be the number of training samples. Let $n_j$ be the number of training samples that are digit $j$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\pi}_j = \\frac{n_j}{n}\n",
    "\\end{align*}\n",
    "\n",
    "The two requirements a probability distribution must fulfill are:\n",
    "\n",
    "1. The probabilities must sum up to $1$\n",
    "\n",
    "2. Every probability $p$ produced by the distribution must be $0 \\leq p \\leq 1$\n",
    "\n",
    "We can verify that $\\hat{\\pi}_j$ is a valid probability distribution. If $n_j$ represents the number samples where digit $j$ appears, then $n_0 + n_1 + ... n_9 = n$, since there are $9$ disjoint subsets of $n$, each with cardinality $n_j$. Due to this fact, convince yourself that $\\hat{\\pi_0} + \\hat{\\pi_1} + ... \\hat{\\pi_9} =1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.09871666666666666, 1: 0.11236666666666667, 2: 0.0993, 3: 0.10218333333333333, 4: 0.09736666666666667, 5: 0.09035, 6: 0.09863333333333334, 7: 0.10441666666666667, 8: 0.09751666666666667, 9: 0.09915}\n"
     ]
    }
   ],
   "source": [
    "import mnist\n",
    "import pickle\n",
    "\n",
    "# Load the training data as a tuple (label, image) where image is a 2D numpy matrix\n",
    "training_data = mnist.read(dataset='training', path='./data')\n",
    "\n",
    "def compute_prior(training_data):\n",
    "    \"\"\"\n",
    "    Generates the prior distribution using training_data\n",
    "    @param training_data The training set provided by the MNIST database\n",
    "    @return A dictionary mapping a digit to the probability that the digit appears in the training set. For example,\n",
    "    prior_distribution[j] = the probability that the digit j appears in the MNIST training set for j in [0,9]\n",
    "    \"\"\"\n",
    "    # The number of times we see each label\n",
    "    label_counts = {}\n",
    "    # The total number of images in the training set\n",
    "    n = 0\n",
    "    # The prior distribution we will compute\n",
    "    prior_distribution = {}\n",
    "\n",
    "    for pair in training_data:\n",
    "        label, image = pair\n",
    "        n+=1\n",
    "        # Increment label_counts[label] for all 0,..., 9 labels\n",
    "        if label in label_counts:\n",
    "            label_counts[label]+=1\n",
    "        else:\n",
    "            label_counts[label] = 1\n",
    "    \n",
    "    for i in range(10):\n",
    "        # Compute the estimation of the prior distribution for all 0, ..., 9 labels\n",
    "        prior_distribution[i] = float(label_counts[i]) / n\n",
    "    \n",
    "    return prior_distribution\n",
    "    \n",
    "prior_distribution = compute_prior(training_data)\n",
    "pickle.dump(prior_distribution, open(\"./data/prior_distribution.p\", \"wb\"))\n",
    "print(prior_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating the Conditional\n",
    "\n",
    "As a reminder we are trying to compute $P_j(x) = P(x \\mid y=j)$, the probability that we generate an image $x$ given that the bin label $y$ is $j$. Each image in the MNIST data set is a 2D matrix with $28$ rows and $28$ columns. We can flatten each of these images into a single dimensional vector with $784$ components. You can think of each image $x$ living in $784$-dimensional space, and each $x$ is a point in that space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADrlJREFUeJzt3X2sVHV+x/HPp6hpxAekpkhYLYsxGDWWTRAbQ1aNYX2I\nRlFjltSERiP7h2zcpCE19I/VtFhTH5qlmg1s1IVmy7qJGtHd+IjKtibEK6IiLuoazUKuUIMo4AOF\n++0fd9je1Tu/ucycmTPwfb+SyZ053zlzvjnhw3mc+TkiBCCfP6u7AQD1IPxAUoQfSIrwA0kRfiAp\nwg8kRfiBpAg/RmX7Rdtf2t7deGyuuydUi/CjZGFEHNN4TK+7GVSL8ANJEX6U/Ivtj23/t+0L6m4G\n1TL39mM0ts+VtEnSXknfl3SfpBkR8ftaG0NlCD/GxPZTkn4dEf9edy+oBrv9GKuQ5LqbQHUIP77B\n9gTbF9v+c9tH2P5bSd+V9FTdvaE6R9TdAPrSkZL+WdLpkvZL+p2kqyLinVq7QqU45geSYrcfSIrw\nA0kRfiApwg8k1dOz/bY5uwh0WUSM6X6Mjrb8ti+xvdn2e7Zv7eSzAPRW25f6bI+T9I6kOZK2SHpF\n0ryI2FSYhy0/0GW92PLPkvReRLwfEXsl/VLSlR18HoAe6iT8UyT9YcTrLY1pf8L2AtsDtgc6WBaA\ninX9hF9ELJe0XGK3H+gnnWz5t0o6ecTrbzWmATgEdBL+VySdZvvbto/S8A8+rK6mLQDd1vZuf0Ts\ns71Q0tOSxkl6MCLeqqwzAF3V02/1ccwPdF9PbvIBcOgi/EBShB9IivADSRF+ICnCDyRF+IGkCD+Q\nFOEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/\nkBThB5Ii/EBShB9IivADSRF+IKm2h+jGoWHcuHHF+vHHH9/V5S9cuLBp7eijjy7OO3369GL95ptv\nLtbvvvvuprV58+YV5/3yyy+L9TvvvLNYv/3224v1ftBR+G1/IGmXpP2S9kXEzCqaAtB9VWz5L4yI\njyv4HAA9xDE/kFSn4Q9Jz9l+1faC0d5ge4HtAdsDHS4LQIU63e2fHRFbbf+lpGdt/y4i1o58Q0Qs\nl7RckmxHh8sDUJGOtvwRsbXxd7ukxyTNqqIpAN3Xdvhtj7d97IHnkr4naWNVjQHork52+ydJesz2\ngc/5z4h4qpKuDjOnnHJKsX7UUUcV6+edd16xPnv27Ka1CRMmFOe95pprivU6bdmypVhfunRpsT53\n7tymtV27dhXnff3114v1l156qVg/FLQd/oh4X9JfV9gLgB7iUh+QFOEHkiL8QFKEH0iK8ANJOaJ3\nN90drnf4zZgxo1hfs2ZNsd7tr9X2q6GhoWL9hhtuKNZ3797d9rIHBweL9U8++aRY37x5c9vL7raI\n8Fjex5YfSIrwA0kRfiApwg8kRfiBpAg/kBThB5LiOn8FJk6cWKyvW7euWJ82bVqV7VSqVe87d+4s\n1i+88MKmtb179xbnzXr/Q6e4zg+giPADSRF+ICnCDyRF+IGkCD+QFOEHkmKI7grs2LGjWF+0aFGx\nfvnllxfrr732WrHe6iesSzZs2FCsz5kzp1jfs2dPsX7mmWc2rd1yyy3FedFdbPmBpAg/kBThB5Ii\n/EBShB9IivADSRF+ICm+z98HjjvuuGK91XDSy5Yta1q78cYbi/Nef/31xfqqVauKdfSfyr7Pb/tB\n29ttbxwxbaLtZ22/2/h7QifNAui9sez2/1zSJV+bdquk5yPiNEnPN14DOIS0DH9ErJX09ftXr5S0\novF8haSrKu4LQJe1e2//pIg4MNjZR5ImNXuj7QWSFrS5HABd0vEXeyIiSifyImK5pOUSJ/yAftLu\npb5ttidLUuPv9upaAtAL7YZ/taT5jefzJT1eTTsAeqXlbr/tVZIukHSi7S2SfizpTkm/sn2jpA8l\nXdfNJg93n332WUfzf/rpp23Pe9NNNxXrDz/8cLE+NDTU9rJRr5bhj4h5TUoXVdwLgB7i9l4gKcIP\nJEX4gaQIP5AU4QeS4iu9h4Hx48c3rT3xxBPFec8///xi/dJLLy3Wn3nmmWIdvccQ3QCKCD+QFOEH\nkiL8QFKEH0iK8ANJEX4gKa7zH+ZOPfXUYn39+vXF+s6dO4v1F154oVgfGBhoWrv//vuL8/by3+bh\nhOv8AIoIP5AU4QeSIvxAUoQfSIrwA0kRfiAprvMnN3fu3GL9oYceKtaPPfbYtpe9ePHiYn3lypXF\n+uDgYLGeFdf5ARQRfiApwg8kRfiBpAg/kBThB5Ii/EBSXOdH0VlnnVWs33vvvcX6RRe1P5jzsmXL\nivUlS5YU61u3bm172Yeyyq7z237Q9nbbG0dMu832VtsbGo/LOmkWQO+NZbf/55IuGWX6v0XEjMbj\nN9W2BaDbWoY/ItZK2tGDXgD0UCcn/H5o+43GYcEJzd5ke4HtAdvNf8wNQM+1G/6fSpomaYakQUn3\nNHtjRCyPiJkRMbPNZQHogrbCHxHbImJ/RAxJ+pmkWdW2BaDb2gq/7ckjXs6VtLHZewH0p5bX+W2v\nknSBpBMlbZP048brGZJC0geSfhARLb9czXX+w8+ECROK9SuuuKJprdVvBdjly9Vr1qwp1ufMmVOs\nH67Gep3/iDF80LxRJj9w0B0B6Cvc3gskRfiBpAg/kBThB5Ii/EBSfKUXtfnqq6+K9SOOKF+M2rdv\nX7F+8cUXN629+OKLxXkPZfx0N4Aiwg8kRfiBpAg/kBThB5Ii/EBShB9IquW3+pDb2WefXaxfe+21\nxfo555zTtNbqOn4rmzZtKtbXrl3b0ecf7tjyA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBSXOc/zE2f\nPr1YX7hwYbF+9dVXF+snnXTSQfc0Vvv37y/WBwfLvxY/NDRUZTuHHbb8QFKEH0iK8ANJEX4gKcIP\nJEX4gaQIP5BUy+v8tk+WtFLSJA0Pyb08In5ie6KkhyVN1fAw3ddFxCfdazWvVtfS580bbSDlYa2u\n40+dOrWdlioxMDBQrC9ZsqRYX716dZXtpDOWLf8+SX8fEWdI+htJN9s+Q9Ktkp6PiNMkPd94DeAQ\n0TL8ETEYEesbz3dJelvSFElXSlrReNsKSVd1q0kA1TuoY37bUyV9R9I6SZMi4sD9lR9p+LAAwCFi\nzPf22z5G0iOSfhQRn9n/PxxYRESzcfhsL5C0oNNGAVRrTFt+20dqOPi/iIhHG5O32Z7cqE+WtH20\neSNieUTMjIiZVTQMoBotw+/hTfwDkt6OiHtHlFZLmt94Pl/S49W3B6BbWg7RbXu2pN9KelPSge9I\nLtbwcf+vJJ0i6UMNX+rb0eKzUg7RPWlS+XTIGWecUazfd999xfrpp59+0D1VZd26dcX6XXfd1bT2\n+OPl7QVfyW3PWIfobnnMHxH/JanZh110ME0B6B/c4QckRfiBpAg/kBThB5Ii/EBShB9Iip/uHqOJ\nEyc2rS1btqw474wZM4r1adOmtdVTFV5++eVi/Z577inWn3766WL9iy++OOie0Bts+YGkCD+QFOEH\nkiL8QFKEH0iK8ANJEX4gqTTX+c8999xifdGiRcX6rFmzmtamTJnSVk9V+fzzz5vWli5dWpz3jjvu\nKNb37NnTVk/of2z5gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiCpNNf5586d21G9E5s2bSrWn3zyyWJ9\n3759xXrpO/c7d+4szou82PIDSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKOiPIb7JMlrZQ0SVJIWh4R\nP7F9m6SbJP1P462LI+I3LT6rvDAAHYsIj+V9Ywn/ZEmTI2K97WMlvSrpKknXSdodEXePtSnCD3Tf\nWMPf8g6/iBiUNNh4vsv225Lq/ekaAB07qGN+21MlfUfSusakH9p+w/aDtk9oMs8C2wO2BzrqFECl\nWu72//GN9jGSXpK0JCIetT1J0scaPg/wTxo+NLihxWew2w90WWXH/JJk+0hJT0p6OiLuHaU+VdKT\nEXFWi88h/ECXjTX8LXf7bVvSA5LeHhn8xonAA+ZK2niwTQKoz1jO9s+W9FtJb0oaakxeLGmepBka\n3u3/QNIPGicHS5/Flh/oskp3+6tC+IHuq2y3H8DhifADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKE\nH0iK8ANJEX4gKcIPJEX4gaQIP5BUr4fo/ljShyNen9iY1o/6tbd+7Uuit3ZV2dtfjfWNPf0+/zcW\nbg9ExMzaGijo1976tS+J3tpVV2/s9gNJEX4gqbrDv7zm5Zf0a2/92pdEb+2qpbdaj/kB1KfuLT+A\nmhB+IKlawm/7Etubbb9n+9Y6emjG9ge237S9oe7xBRtjIG63vXHEtIm2n7X9buPvqGMk1tTbbba3\nNtbdBtuX1dTbybZfsL3J9lu2b2lMr3XdFfqqZb31/Jjf9jhJ70iaI2mLpFckzYuITT1tpAnbH0ia\nGRG13xBi+7uSdktaeWAoNNv/KmlHRNzZ+I/zhIj4hz7p7TYd5LDtXeqt2bDyf6ca112Vw91XoY4t\n/yxJ70XE+xGxV9IvJV1ZQx99LyLWStrxtclXSlrReL5Cw/94eq5Jb30hIgYjYn3j+S5JB4aVr3Xd\nFfqqRR3hnyLpDyNeb1GNK2AUIek526/aXlB3M6OYNGJYtI8kTaqzmVG0HLa9l742rHzfrLt2hruv\nGif8vml2RMyQdKmkmxu7t30pho/Z+ula7U8lTdPwGI6Dku6ps5nGsPKPSPpRRHw2slbnuhulr1rW\nWx3h3yrp5BGvv9WY1hciYmvj73ZJj2n4MKWfbDswQnLj7/aa+/mjiNgWEfsjYkjSz1TjumsMK/+I\npF9ExKONybWvu9H6qmu91RH+VySdZvvbto+S9H1Jq2vo4xtsj2+ciJHt8ZK+p/4beny1pPmN5/Ml\nPV5jL3+iX4ZtbzasvGped3033H1E9Pwh6TINn/H/vaR/rKOHJn1Nk/R64/FW3b1JWqXh3cD/1fC5\nkRsl/YWk5yW9K+k5SRP7qLf/0PBQ7m9oOGiTa+pttoZ36d+QtKHxuKzudVfoq5b1xu29QFKc8AOS\nIvxAUoQfSIrwA0kRfiApwg8kRfiBpP4PdQK+Ne/X5oUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1154955c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "training_data = mnist.read(dataset='training', path='./data')\n",
    "label, image = training_data.__next__()\n",
    "plt.title(label)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each pixel $x_i \\in \\{0, ..., 255\\}$ because this is a 8-bit grayscale image. Since we will eventually be dealing with probabilities it will be easier to deal with grayscale images where $x\\in [0,1]^{784}$ and each pixel $x_i \\in [0,1]$. Furthermore, we can threshold the the image by setting each $x_i=1$ if $x_i \\geq \\frac{1}{2}$ and $x_i=0$ otherwise. This method is known as *binarization* of an image. This gives us the ability to treat each pixel $x_i$ as a Bernoulli random variable, or indicator random variable, since they can only take on values $1$ or $0$. This gives us an image $x \\in \\{0,1\\}^{784}$ and $x_i \\in \\{0,1\\}$. Note the difference between the square brackets in the first sentence and the curly brackets in the previous sentence.\n",
    "\n",
    "Here's where the \"naive\" part comes in. We can treat each pixel in the image of a digit as independent random variables. Recall that if two events $A$ and $B$ are independent, $P(A \\cap B) = P(A)P(B)$. We can use this very simple fact to make the computation of $P_j(x)$ very easy.\n",
    "\n",
    "\\begin{align*}\n",
    "P_j(x) = P_{j1}(x_1) \\cdot P_{j2}(x_2) \\cdot \\cdot \\cdot P_{j,784}(x_{784})\n",
    "\\end{align*}\n",
    "\n",
    "We can parameterize $P_j(x)$ with only $784$ values as opposed to $2^{784}$ values if we didn't make the assumption that each pixel $x_i$ was independent. Let $P_{ji}(x)$ be the probability that the $i$-th pixel of digit $j$ is $1$.\n",
    "\n",
    "\\begin{align*}\n",
    "P_{ji}(x_i=1) = p_{ji} \\\\\n",
    "P_{ji}(x_i=0) = 1-p_{ji} \\\\\n",
    "P_{ji}(x_i) = p^{x_i}_{ji}(1-p_{ji})^{1-x_i}, \\,x_i \\in \\{0,1\\}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is more clear that $x_i \\sim Ber(p_{ji})$ and $P_j(x)$ is just a product of $784$ independent random variables. More importantly, it represents the probability that pixels that are \"on\", or take on the value $1$ form the digit $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $n$ be the number of training samples. Let $n_j$ be the number of samples with digit $j$. Let $n_{ji}$ be the number of samples of digit $j$ that have the pixel $x_i=1$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{p}_{ji} = \\frac{n_{ji}}{n_j}\n",
    "\\end{align*}\n",
    "\n",
    "This is the estimated probability of the $i$-th pixel in digit $j$ being $1$. This estimation is computed directly from the training data. There's a small problem with this though. If $n_{ji} = 0$, or we see no images of digit $j$ with the $i$-th pixel equal to $1$, then $\\hat{p}_{ji}$ will be $0$. Similarly if all training images of digit $j$ have the $1$ in the $i$-th pixel, then $n_{ji} = n_j$, so $\\hat{p}_{ji}$ will be $1$. Although this is representative of our current training samples, this may not represent an image we have not seen before. To account for this, we will artificially add two training images to the set. One with $x_i=0$ and $x_i=1$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{p}_{ji} = \\frac{n_{ji} + 0 + 1}{n_j + 2} \\\\\n",
    "\\tilde{p}_{ji} = \\frac{n_{ji}+1}{n_j + 2}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{arg}\\,\\operatorname{max}}\\;}$\n",
    "\n",
    "Recall the $h(x) = \\arg\\max_{j} \\pi_jP_j(x)$ and we were trying to find $\\pi_j$ and $P_j(x)$. We have found both, so we can start rewriting this equation in more concrete terms.\n",
    "\n",
    "\\begin{align*}\n",
    "P_j(x) = \\prod_{i=0}^{783} P_{ji}(x_i) = \\prod_{i=0}^{783} p^{x_i}_{ji}(1-p_{ji})^{1-x_i}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "h(x) &= \\arg\\max_{j} \\pi_jP_j(x)\\\\\n",
    "      &= \\arg\\max_{j} \\pi_j \\prod_{i=0}^{783} p^{x_i}_{ji}(1-p_{ji})^{1-x_i}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\"Underflow\" occurs when a computer cannot represent a floating point number beyond some specified precision. Multiplying a several small decimals together (ie. probabilities), can cause this. Estimating $h(x)$ above will cause Python to error out. Instead, we will maximize the function below. The logarithm has the effect of taking a decimal $d$ and outputing $-\\frac{1}{d}$ which produces numbers with a whole number part and a decimal part, avoiding underflow. The important part is that even though the maximums for each $j \\in \\{0, ..., 9\\}$ will be different, the $j$ that maximizes $h(x)$ will be the same, because $log$ is a monotonically increasing function.\n",
    "\n",
    "\\begin{align*}\n",
    "h(x) &= \\arg\\max_{j} log(\\pi_jP_j(x)) \\\\\n",
    "     &= \\arg\\max_{j} log(\\pi_j \\prod_{i=0}^{783} p^{x_i}_{ji}(1-p_{ji})^{1-x_i}) \\\\\n",
    "     &= \\arg\\max_{j} log(\\pi_j) + log(p^{x_0}_{j0}) + log((1-p_{j0})^{1-x_0}) + ... + log(p^{x_{783}}_{j783}) + log((1-p_{j783})^{1-x_{783}}) \\\\\n",
    "     &= \\arg\\max_{j} log(\\pi_j) + x_0log(p_{j0}) + (1-x_0)log(1-p_{j0}) + ... + x_{783}log(p_{j783}) + (1-x_{783})log(1-p_{j783}) \\\\\n",
    "     &= \\arg\\max_{j} log(\\pi_j) + \\sum_{i=0}^{783} (x_i\\,log\\,p_{ji} + (1-x_i)log(1-p_{ji}))\n",
    "\\end{align*}\n",
    "\n",
    "Below is a program to compute the pixel probabilities $\\tilde{p}_{ji}$ for all $i \\in \\{0, ..., 783\\}$ for all $j \\in \\{0, ..., 9\\}$. This program will take a while to run depending on your processor clock speed, which is why this notebook comes with the pre-computed distributions in `data/pixel_probabilities.p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some common image processing functions\n",
    "\n",
    "def normalize_image(image):\n",
    "    \"\"\"\n",
    "    Normalizes a flattened image\n",
    "    @param image A flattened image\n",
    "    \"\"\"\n",
    "    image = 1/255 * image\n",
    "\n",
    "def binarize_image(image):\n",
    "    \"\"\"\n",
    "    Applies a fixed threshold on a flattened image. For each pixel x_i, \n",
    "    x_i = 1 if x_i >= 1/2 and x_i = 0 otherwise\n",
    "    @param image A flattened image\n",
    "    \"\"\"\n",
    "    for i in range(0, len(image)):\n",
    "        if image[i] >= 0.5:\n",
    "            image[i] = 1\n",
    "        else:\n",
    "            image[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "training_data = mnist.read(dataset='training', path='./data')\n",
    "\n",
    "def estimate_pixel_probabilities(training_data):\n",
    "    \"\"\"\n",
    "    Generates the pixel probabilities using training_data\n",
    "    @param training_data The training set provided by the MNIST database\n",
    "    @return A dictionary mapping a digit to an array of pixel probabilities. For example, \n",
    "    pixel_probabilities[j][i] = probability that the i-th pixel is 1 in the j-th digit for j in [0,9] and i in [0,783]\n",
    "    \"\"\"\n",
    "    # pixel_counts[j][i] = number of times the i-th pixel is 1 in the j-th digit for j in [0,9] and i in [0,783]\n",
    "    pixel_counts = {}\n",
    "    # pixel_probabilities[j][i] = probability that the i-th pixel is 1 in the j-th digit\n",
    "    pixel_probabilities = {}\n",
    "    # label_counts[j] = number of times digit j appears in the training set\n",
    "    label_counts = {}\n",
    "    # the prior distribution generated in compute_prior()\n",
    "    prior_distribution = pickle.load(open('./data/prior_distribution.p', 'rb'))\n",
    "    \n",
    "    # Compute the pixel_counts and label_counts for every digit in the training set\n",
    "    for pair in training_data:\n",
    "        label, _image = pair\n",
    "        image = _image.flatten()\n",
    "        normalize_image(image)\n",
    "        binarize_image(image)\n",
    "            \n",
    "        if label not in label_counts:\n",
    "            label_counts[label] = 1\n",
    "        else:\n",
    "            label_counts[label]+=1\n",
    "            \n",
    "        if label not in pixel_counts:\n",
    "            pixel_counts[label] = [0] * 784\n",
    "        \n",
    "        for i in range(len(image)):\n",
    "            if image[i] == 1:\n",
    "                pixel_counts[label][i] +=1\n",
    "    \n",
    "    # Compute the pixel_probabilities using label_counts and pixel_counts\n",
    "    for j in range(10):\n",
    "        n_j = label_counts[j]\n",
    "        pixel_probabilities[j] = [0] * 784\n",
    "        for i in range(784):\n",
    "            n_ji = pixel_counts[j][i]\n",
    "            p_ji = float(n_ji + 1) / (n_j + 2)\n",
    "            pixel_probabilities[j][i] = p_ji\n",
    "\n",
    "    return pixel_probabilities\n",
    "\n",
    "pixel_probabilities = estimate_pixel_probabilities(training_data)\n",
    "pickle.dump(pixel_probabilities, open(\"./data/pixel_probabilities.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Validation\n",
    "\n",
    "The MNIST data comes with a set of test imagery $(x', y')$ where $x'$ is an image that is not in the training set, and $y'$ is its corresponding label. We can measure the accuracy of our MAP estimator by seeing how many of the $x'$ images we classify correctly. But first, we must first code our MAP estimator. Since we have computed the prior distribution and the pixel probabilities for each digit, we just have to compute the conditional probability given an image and find the digit $j$ that maximizes their product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist\n",
    "import pickle\n",
    "from math import log\n",
    "\n",
    "prior_distribution = pickle.load(open(\"./data/prior_distribution.p\", \"rb\"))\n",
    "pixel_probabilities = pickle.load(open(\"./data/pixel_probabilities.p\", \"rb\"))\n",
    "\n",
    "def estimate_digit(image):\n",
    "    \"\"\"\n",
    "    Given a (28,28) grayscale image, estimates which digit [0,...,9] is written in it\n",
    "    @param image A (28,28) grayscale image\n",
    "    @return The digit [0,...,9] which is most likely to be written inside of the image\n",
    "    \"\"\"\n",
    "    image = image.flatten()\n",
    "    normalize_image(image)\n",
    "    binarize_image(image)\n",
    "    best_digit = None\n",
    "    best_max = None\n",
    "    \n",
    "    # h(x) function, but in code\n",
    "    # Since we are working over a small discrete space [0,...,9], we can simply\n",
    "    # loop through all 10 digits and see which digit maximizes h(x)\n",
    "    for j in range(10):\n",
    "        s = log(prior_distribution[j])\n",
    "        for i in range(len(image)):\n",
    "            x_i = image[i]\n",
    "            p_ji = pixel_probabilities[j][i]\n",
    "            s+= x_i * log(p_ji) + (1-x_i) * log(1-p_ji)\n",
    "        if best_max is None or s > best_max:\n",
    "            best_max = s\n",
    "            best_digit = j\n",
    "    return best_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data = mnist.read(dataset='testing', path='./data')\n",
    "\n",
    "def measure_accuracy(test_data):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for pair in test_data:\n",
    "        label, image = pair\n",
    "        if (estimate_digit(image)) == label:\n",
    "            correct+=1\n",
    "        total+=1\n",
    "        print('.')\n",
    "    return float(correct) / total\n",
    "\n",
    "accuracy = measure_accuracy(test_data)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Whether you take my word, or run the above cell, the accuracy of the MAP estimator on the MNIST dataset is $0.8413$ or $84.13\\%$ which is close to the accuracy reported in Note 21, $84.6\\%$. The difference is likely due to how different programming languages handle floating point multiplication and division, but that's just a guess. Intuitively it means that $8,413$ out of $10,000$ test images were classified correctly. The MAP estimator gets a B in accuracy, but it's very easy to implement and run compared to more sophisticated techniques like neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
